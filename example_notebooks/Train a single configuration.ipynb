{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "Uncomment and install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U message-passing-nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the repository to get the data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/kovanostra/message-passing-nn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "from message_passing_nn.model.model_trainer import ModelTrainer\n",
    "from message_passing_nn.graph.graph_rnn_encoder import GraphRNNEncoder\n",
    "from message_passing_nn.graph.graph_gru_encoder import GraphGRUEncoder\n",
    "from message_passing_nn.data.data_preprocessor import DataPreprocessor\n",
    "from message_passing_nn.repository.file_system_repository import FileSystemRepository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"gpu\" # You can use \"cuda\" for GraphRNNEncoder, but it is currently adviced to use \"cpu\" for the GraphGRUEncoder\n",
    "epochs = 10\n",
    "model = 'RNN'\n",
    "loss_function = 'MSE'\n",
    "optimizer = 'SGD'\n",
    "batch_size = 5\n",
    "maximum_number_of_nodes = 250 # Some of the adjacency matrices in our dataset are too big, this variable controls the maximum size of the matrices to load. To load the whole dataset set this value to -1.\n",
    "maximum_number_of_features = 10 # Similarly for the number of features\n",
    "validation_split = 0.2\n",
    "test_split = 0.1\n",
    "time_steps = 1 # The time steps of the message passing algorithm\n",
    "validation_period = 20\n",
    "\n",
    "configuration_dictionary = {'time_steps': time_steps,\n",
    "                            'model': model,\n",
    "                            'loss_function': loss_function,\n",
    "                            'optimizer': optimizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerocess the dataset\n",
    "We load the protein-folding datacet in which each graph contains three pickle files:\n",
    "  1. The features of each node (as torch.tensor.Size([M,N]))\n",
    "  2. The adjacency matrix (as torch.tensor.Size([M,M]))\n",
    "  3. The labels to predict (as torch.tensor.Size([L]))\n",
    "\n",
    "where M is the number of graph nodes, N the number of features per node, and L the number of values to predict.\n",
    "\n",
    "The dataset contains features and labels from 31 proteins from (https://www.rcsb.org). We apply a limit to the size of the proteins (to not crush the runtime) to we end up with 17 proteins which we equalize in size and split into training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'protein-folding'\n",
    "data_directory = 'message-passing-nn/data/'\n",
    "file_system_repository = FileSystemRepository(data_directory, dataset_name)\n",
    "raw_dataset = file_system_repository.get_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please uncomment the following block to see examples of the data used as input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_features_example, adjacency_matrix_example, labels_example = raw_dataset[0]\n",
    "# print(node_features_example.size(), adjacency_matrix_example.size(), labels_example.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we equalize the tensor sizes and split to train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessor = DataPreprocessor()\n",
    "equalized_dataset = data_preprocessor.equalize_dataset_dimensions(raw_dataset,\n",
    "                                                                  maximum_number_of_nodes,\n",
    "                                                                  maximum_number_of_features)\n",
    "training_data, validation_data, test_data = data_preprocessor.train_validation_test_split(equalized_dataset, \n",
    "                                                                                          batch_size, \n",
    "                                                                                          validation_split, \n",
    "                                                                                          test_split)\n",
    "data_dimensions = data_preprocessor.extract_data_dimensions(equalized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model and the trainer\n",
    "\n",
    "The Trainer is responsible for the instantiation, training and evaluation of the model. It also controls whether a mini-batch normalization over the node features and labels should be applied. The ModelTrainer can use either the RnnEncoder or the GRUEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_dictionary = {'time_steps': time_steps,\n",
    "                            'model': model,\n",
    "                            'loss_function': loss_function,\n",
    "                            'optimizer': optimizer}\n",
    "model_trainer = ModelTrainer(data_preprocessor, device)\n",
    "model_trainer.instantiate_attributes(data_dimensions, configuration_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "This block will train the model and output the training, validation and test losses along with the time. Our use case contains fully connected graphs and therefore the time to train is significantly longer than for sparsely connected graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    training_loss = model_trainer.do_train(training_data, epoch)\n",
    "    print(\"Epoch\", epoch, \"Training loss:\", training_loss)\n",
    "    if epoch % validation_period == 0:\n",
    "        validation_loss = model_trainer.do_evaluate(validation_data, epoch)\n",
    "        print(\"Epoch\", epoch, \"Validation loss:\", validation_loss)\n",
    "test_loss = model_trainer.do_evaluate(test_data)\n",
    "print(\"Test loss:\", validation_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
